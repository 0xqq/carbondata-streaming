carbon.schema.maxFileSize=50
dataload.taskstatus.retention=2
carbon.numberOfCubesToLoadConcurrent=5
max.memory.threshold=60
min.memory.threshold=50
#1 means every day
carbon.retention.schedule=1
carbon.dataload.queuesize=100
carbon.dataload.concurrent.execution.size=1
carbon.result.limit=100000000
mysql.null.value=\\N
mssql.null.value=
oracle.null.value=NULL
carbon.sort.size=1000000
carbon.queryexecutor.concurrent.execution.size=3
#################### EXECUTION THREADS ##################
carbon.number.of.cores=4
carbon.smartJump.avoid.percent=70
carbon.agg.enableXXHash=true
carbon.spark.resultlimit=20000
carbon.cache.used=false

mysql.resultset.cursor.moveonly.forward=false

carbon.level.write.bufferinkb=12238
carbon.graph.rowset.size=100000
carbon.sort.file.write.buffer.size=10485760
carbon.sort.intermediate.files.limit=50
carbon.sort.file.buffer.size=20
carbon.sort.intermedaite.number.of.therads=5
carbon.csv.read.buffersize.byte=1048576
carbon.csv.read.copies=6
carbon.datawriter.write.all.node=true
carbon.data.load.log.counter=500000
carbon.number.of.cores.while.loading=6
carbon.prefetch.in.merge=true
carbon.prefetch.bufferSize=20000
carbon.inmemory.cache.use=true
carbon.dataload.log.enabled=true


## Spark CARBON related Properties
#spark.dataset.location=../datasets_test/
#spark.dp.location=../datapipelines_test/
#spark.sqlconnections.location=../unibi-solutions/system/dbconnection/sqlconnections_test.xml
#spark.url=local

#carbon.storelocation=hdfs://master:54310/opt/ravi/store
#carbon.storelocation=/opt/ravi/store1day
carbon.storelocation=hdfs://master:54310/opt/ravi/perfstore
#carbon.storelocation=/opt/ravi/store1day
#carbon.storelocation=/opt/ravi/storebasecarbon
#carbon.storelocation=/opt/ravi/storesinglenode




spark.dataset.location=hdfs://master:54310/opt/ravi/sparkcarbon/datasets/
spark.dp.location=hdfs://master:54310/opt/ravi/sparkcarbon/datapipelines/
spark.sqlconnections.location=hdfs://master:54310/opt/ravi/sparkcarbon/sqlconnections/sqlconnections_test.xml
spark.url=spark://master:7077
spark.home=/opt/spark-1.0.0-rc3
#spark.schema.path=/opt/ravi/steelwheels.carbon.xml
spark.schema.path=/opt/ravi/PCC_Java.xml
spark.schema.name=PCC
spark.cube.name=ODM

spark.executor.memory=200g
spark.cores.max=76
spark.usekryo.serializer=true
spark.eventLog.enabled=true
spark.sql.shuffle.partitions=200

##### New properties for columnar ####################################################################
# Enbale Columnar
carbon.is.columnar.storage=true
#Int or Short based indexes. use Int now (TODO  Short is not working) 
is.int.based.indexer=true			
#Store Unique Values for a column if not high cardinality dimension 
aggregate.columnar.keyblock=true
#Threshold for a dimension be considered High Cardinality 
high.cardinality.value=100000
#Numbers of tuples in Leaf  ( this can be 15x for columar store comared to row based store since each column is sperately read/decompressed) 
carbon.leaf.node.size=120000
#Use multiple of 8 bits for a colmn value
carbon.is.fullyfilled.bits=true
#To use NumberCompressor.java for compression . Since no benefit was found, keep it false
is.compressed.keyblock=false
#How many levels will be combined into one column .TODO only one supported
carbon.dimension.split.value.in.columnar=1
